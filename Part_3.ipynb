{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b6ebe3-b4ab-4b5a-8d8f-c4ca54ef9387",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting xgboost\n  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\nCollecting nvidia-nccl-cu12\n  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.7.3)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.21.5)\nInstalling collected packages: nvidia-nccl-cu12, xgboost\nSuccessfully installed nvidia-nccl-cu12-2.23.4 xgboost-2.1.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d133323e-0e60-4210-8730-8002f3c83ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "destination_path = \"/dbfs/mnt/bde-assignment2/nyc_taxi_final.parquet\"\n",
    "\n",
    "nyc_taxi = spark.read.parquet(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f54c15-1b7c-418c-92fe-fc08b0d1a956",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import avg, col\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b84efb2-29f5-45cd-ba92-957d7f182b6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "green_taxi = nyc_taxi.filter(F.col(\"taxi_type\") == \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ca670f-d794-43e4-b1a3-39a5a1768415",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DOLocationID: long (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- VendorID: long (nullable = true)\n |-- pickup_datetime: timestamp (nullable = true)\n |-- dropoff_datetime: timestamp (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- payment_type: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- trip_distance_km: double (nullable = true)\n |-- trip_duration: double (nullable = true)\n |-- speed_kmh: double (nullable = true)\n |-- taxi_type: string (nullable = true)\n |-- trip_type: double (nullable = true)\n |-- ehail_fee: double (nullable = true)\n |-- pickup_borough: string (nullable = true)\n |-- pickup_zone: string (nullable = true)\n |-- pickup_service_zone: string (nullable = true)\n |-- dropoff_borough: string (nullable = true)\n |-- dropoff_zone: string (nullable = true)\n |-- dropoff_service_zone: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "green_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892bafac-8f68-40e5-ba7c-b22a6542e24a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: 63421542"
     ]
    }
   ],
   "source": [
    "green_taxi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7565cca-5df9-4c39-b546-573fd7b0adcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----------------+-------------+---------+---------+---------+---------+--------------+-----------+-------------------+---------------+------------+--------------------+\n|DOLocationID|PULocationID|VendorID|pickup_datetime|dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|trip_distance_km|trip_duration|speed_kmh|taxi_type|trip_type|ehail_fee|pickup_borough|pickup_zone|pickup_service_zone|dropoff_borough|dropoff_zone|dropoff_service_zone|\n+------------+------------+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----------------+-------------+---------+---------+---------+---------+--------------+-----------+-------------------+---------------+------------+--------------------+\n|           0|           0|       0|              0|               0|        1745567|            0|   1745567|           1745567|     1745567|          0|    0|      0|         0|           0|                    1|           0|            56211965|   63421542|               0|            0|        0|        0|  1748497| 63418617|             0|          0|                  0|              0|           0|                   0|\n+------------+------------+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----------------+-------------+---------+---------+---------+---------+--------------+-----------+-------------------+---------------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "null_counts = green_taxi.select(\n",
    "    [F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in green_taxi.columns]\n",
    ")\n",
    "\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea7a64db-188d-46fc-9914-6128363bece7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 1 - Green taxi dataset with Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04db7c11-e672-486e-abf1-83995649cf54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gt_cleaned = green_taxi\n",
    "\n",
    "drop_columns = [\"mta_tax\", \"tolls_amount\", \"store_and_fwd_flag\", \"improvement_surcharge\", \"RatecodeID\" ,\"congestion_surcharge\", \"airport_fee\", \"speed_kmh\", \"trip_type\", \"ehail_fee\", \"payment_type\", \"VendorID\", \"fare_amount\", \"taxi_type\"]\n",
    "                \n",
    "drop_point_columns = ['pickup_borough', 'pickup_zone', 'pickup_service_zone', 'dropoff_borough', 'dropoff_zone', 'dropoff_service_zone']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f93e23-3cf2-462e-84a0-833d5e39ee83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gt_cleaned = gt_cleaned.drop(*drop_columns)\n",
    "gt_cleaned = gt_cleaned.drop(*drop_point_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3bdbb0-53cb-4b8f-8c3b-b24cb30baa7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gt_cleaned = gt_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfb8a49-7b79-4350-a834-95105b2ee574",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------+----------------+---------------+-------------+-----+----------+------------+----------------+-------------+---------+\n|DOLocationID|PULocationID|pickup_datetime|dropoff_datetime|passenger_count|trip_distance|extra|tip_amount|total_amount|trip_distance_km|trip_duration|taxi_type|\n+------------+------------+---------------+----------------+---------------+-------------+-----+----------+------------+----------------+-------------+---------+\n|           0|           0|              0|               0|              0|            0|    0|         0|           0|               0|            0|        0|\n+------------+------------+---------------+----------------+---------------+-------------+-----+----------+------------+----------------+-------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "null_counts = gt_cleaned.select(\n",
    "    [F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in gt_cleaned.columns]\n",
    ")\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21ed518-6ce5-49e5-9af8-7436e04fe7ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, hour, dayofweek, expr\n",
    "\n",
    "gt_cleaned = gt_cleaned.withColumn(\"year\", year(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"month\", month(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"hour\", hour(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"weekday\", dayofweek(\"pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1806c56b-1680-44b3-ac0d-e570a2142f5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gt_cleaned = gt_cleaned.drop(\"pickup_datetime\", \"dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04682361-9a3f-451e-a2d4-b5a4f8aa4dd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DOLocationID', 'PULocationID', 'passenger_count', 'trip_distance', 'extra', 'tip_amount', 'total_amount', 'trip_distance_km', 'trip_duration', 'year', 'month', 'hour', 'weekday']\n"
     ]
    }
   ],
   "source": [
    "#cat_cols = ['store_and_fwd_flag']\n",
    "num_cols = [field.name for field in gt_cleaned.schema.fields if field.dataType.typeName() in [\"integer\", \"double\", \"long\"]]\n",
    "\n",
    "# Display the list\n",
    "print(num_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c44127-1916-4869-a044-f56375177f82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " * Add Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53b89ae0-6951-4811-91ed-86337233301a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c23d56-8777-49b5-bbc1-e3d1cf109b06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_indexer = StringIndexer(inputCol='total_amount', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a15232-04db-481c-94c7-d7e6c7c15925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=num_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ea1993-4526-470d-9d4a-9db8fce8d071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stages += [target_indexer, assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905da7a9-913c-4530-8f3b-2cb8b2da17ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(gt_cleaned)\n",
    "gt_cleaned = pipeline_model.transform(gt_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd4b11db-d705-4ffd-82fd-ad95388f6c0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train and validation data (all data except October, November, December 2022)\n",
    "gt_cleaned_filtered = gt_cleaned.filter((col(\"year\") != 2022) | ((col(\"year\") == 2022) & (col(\"month\") < 10)))\n",
    "\n",
    "# Test data (October, November, December 2022)\n",
    "gt_cleaned_test = gt_cleaned.filter((col(\"year\") == 2022) & (col(\"month\").between(10, 12)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d137af3-634f-4532-b796-660153cb4306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gt_train, gt_val = gt_cleaned_filtered.randomSplit([0.8, 0.2], seed=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed01de7-3960-40ea-8939-2297b3a30663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data = gt_train.sample(fraction=0.1)\n",
    "val_data = gt_val.sample(fraction=0.1)\n",
    "\n",
    "train_data = train_data.repartition(10)\n",
    "val_data = val_data.repartition(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d019f8c-5d73-4cf8-adab-4140b774a427",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " - Build baseline model using lit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c630d4c3-0960-4847-91e5-45f16a605069",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 557.599952232893\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Calculate the baseline average total_amount\n",
    "baseline_value = train_data.select(avg(\"total_amount\")).first()[0]\n",
    "\n",
    "# Create a DataFrame with baseline predictions\n",
    "baseline_predictions = train_data.withColumn(\"prediction\", lit(baseline_value))\n",
    "\n",
    "# Calculate RMSE for baseline model\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "baseline_rmse = evaluator.evaluate(baseline_predictions)\n",
    "\n",
    "# Print the baseline RMSE\n",
    "print(f\"Baseline RMSE: {baseline_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a6502a-a8d2-41ba-995e-c2c28a1568d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set RMSE: 443.9566476347632\nValidate set RMSE: 440.0376251042335\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='label', maxIter=10) \n",
    "\n",
    "lr_model = lr.fit(train_data)\n",
    "train_predictions = lr_model.transform(train_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Evaluate train RMSE\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "print(f\"Train set RMSE: {train_rmse}\")\n",
    "val_predictions = lr_model.transform(val_data)\n",
    "\n",
    "# Evaluate Val RMSE\n",
    "val_rmse = evaluator.evaluate(val_predictions)\n",
    "print(f\"Validate set RMSE: {val_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66b800a2-9447-4f36-bed9-97035fb53310",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 2 - Yellow taxi dataset (2021~2022.9) with Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0a682d-fb33-4f47-a3ac-e0dd5b729284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yellow_taxi = nyc_taxi.filter(F.col(\"taxi_type\") == \"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef62b3d-182d-46cc-a73c-112cf42c071c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yt_cleaned = yellow_taxi\n",
    "\n",
    "drop_columns = [\"mta_tax\", \"tolls_amount\", \"store_and_fwd_flag\", \"improvement_surcharge\", \"RatecodeID\" ,\"congestion_surcharge\", \"airport_fee\", \"speed_kmh\", \"trip_type\", \"ehail_fee\", \"payment_type\", \"VendorID\", \"fare_amount\", \"taxi_type\"]\n",
    "                \n",
    "drop_point_columns = ['pickup_borough', 'pickup_zone', 'pickup_service_zone', 'dropoff_borough', 'dropoff_zone', 'dropoff_service_zone']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826a1477-33ee-486a-bca4-f052445c0cf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yt_cleaned = yt_cleaned.drop(*drop_columns)\n",
    "yt_cleaned = yt_cleaned.drop(*drop_point_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591b0d6a-97c4-4ae1-b852-acd6ef72996b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yt_cleaned = yt_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4bf113-d99a-46ca-bf9e-50fa96009759",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, hour, dayofweek, expr\n",
    "\n",
    "yt_cleaned = yt_cleaned.withColumn(\"year\", year(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"month\", month(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"hour\", hour(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"weekday\", dayofweek(\"pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916f505d-ef6e-4715-9898-e4c8def58548",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yt_cleaned = yt_cleaned.drop(\"pickup_datetime\", \"dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208a2603-cdf7-474b-9729-40828f7a6673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yt_cleaned_2021 = yt_cleaned.filter((col(\"year\") == 2021) | ((col(\"year\") == 2022) & (col(\"month\") < 10)))\n",
    "yt_cleaned_2019 = yt_cleaned.filter((col(\"year\") >= 2019) & ((col(\"year\") <= 2020)))\n",
    "yt_cleaned_2016 = yt_cleaned.filter((col(\"year\") >= 2016) & ((col(\"year\") <= 2018)))\n",
    "yt_cleaned_2015 = yt_cleaned.filter((col(\"year\") == 2015))\n",
    "\n",
    "# Test data (October, November, December 2022)\n",
    "yt_cleaned_test = yt_cleaned.filter((col(\"year\") == 2022) & (col(\"month\").between(10, 12)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480d6fbb-ee34-42b1-b9e3-e4ef04af20a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "num_cols = [field.name for field in yt_cleaned.schema.fields if field.dataType.typeName() in [\"integer\", \"double\", \"long\"]]\n",
    "\n",
    "stages = []\n",
    "\n",
    "target_indexer = StringIndexer(inputCol='total_amount', outputCol='label')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=num_cols, outputCol=\"features\")\n",
    "\n",
    "stages += [target_indexer, assembler]\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(yt_cleaned_2021)\n",
    "yt_cleaned_2021 = pipeline_model.transform(yt_cleaned_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8fb143-4483-4ea4-823a-88bad4f0ccbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(yt_cleaned_test)\n",
    "yt_cleaned_test = pipeline_model.transform(yt_cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f484329e-77f1-457c-af63-6c59c09f0f9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yt_train, yt_val = yt_cleaned_2021.randomSplit([0.8, 0.2], seed=8)\n",
    "\n",
    "train_data_yt = yt_train.sample(fraction=0.1)\n",
    "val_data_yt = yt_val.sample(fraction=0.1)\n",
    "\n",
    "train_data_yt = train_data_yt.repartition(10)\n",
    "val_data_yt = val_data_yt.repartition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d9624e-2852-4029-ad3a-a62b4e5fc786",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set RMSE: 490.18584634597516\nValidate set RMSE: 486.0378436001615\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='label', maxIter=10) \n",
    "\n",
    "lr_model = lr.fit(train_data_yt)\n",
    "train_predictions_yt = lr_model.transform(train_data_yt)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Evaluate train RMSE\n",
    "train_rmse = evaluator.evaluate(train_predictions_yt)\n",
    "print(f\"Train set RMSE: {train_rmse}\")\n",
    "val_predictions_yt = lr_model.transform(val_data_yt)\n",
    "\n",
    "# Evaluate Val RMSE\n",
    "val_rmse = evaluator.evaluate(val_predictions_yt)\n",
    "print(f\"Validate set RMSE: {val_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e876827-6682-4f02-89e9-3fe389fd321e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 611.979028414324\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Calculate the baseline average total_amount\n",
    "baseline_value = train_data_yt.select(avg(\"total_amount\")).first()[0]\n",
    "\n",
    "# Create a DataFrame with baseline predictions\n",
    "baseline_predictions = train_data_yt.withColumn(\"prediction\", lit(baseline_value))\n",
    "\n",
    "# Calculate RMSE for baseline model\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "baseline_rmse = evaluator.evaluate(baseline_predictions)\n",
    "\n",
    "# Print the baseline RMSE\n",
    "print(f\"Baseline RMSE: {baseline_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57eb3df7-3933-4588-87dd-0fd904f9a472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 641.5817413624575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_predictions_yt = lr_model.transform(yt_cleaned_test)\n",
    "\n",
    "test_rmse = evaluator.evaluate(test_predictions_yt)\n",
    "print(f\"Test set RMSE: {test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "212fdad8-ad3a-409c-ba77-0d792bd3316d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 3 - Yellow taxi dataset (2021~2022.9) with Xgboosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c674be9-8c6b-4091-aa5d-11f017ee8e76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 06:52:20,772 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-01 07:09:01,625 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1882788124453729>:6\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m xgb_regressor \u001B[38;5;241m=\u001B[39m SparkXGBRegressor(features_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, label_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m xgb_model \u001B[38;5;241m=\u001B[39m xgb_regressor\u001B[38;5;241m.\u001B[39mfit(train_data_yt)\n",
       "\u001B[0;32m----> 6\u001B[0m xgb_predictions \u001B[38;5;241m=\u001B[39m xgb_model\u001B[38;5;241m.\u001B[39mtransform(yt_cleaned_test)\n",
       "\u001B[1;32m      8\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m RegressionEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m xgb_rmse \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(xgb_predictions)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n",
       "\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e3d10686-b892-4abb-a373-bf46e7d14bca/lib/python3.9/site-packages/xgboost/spark/core.py:1354\u001B[0m, in \u001B[0;36m_SparkXGBModel._transform\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m     base_margin_col \u001B[38;5;241m=\u001B[39m col(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetOrDefault(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_margin_col))\u001B[38;5;241m.\u001B[39malias(\n",
       "\u001B[1;32m   1350\u001B[0m         alias\u001B[38;5;241m.\u001B[39mmargin\n",
       "\u001B[1;32m   1351\u001B[0m     )\n",
       "\u001B[1;32m   1352\u001B[0m has_base_margin \u001B[38;5;241m=\u001B[39m base_margin_col \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 1354\u001B[0m features_col, feature_col_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_feature_col\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1355\u001B[0m enable_sparse_data_optim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetOrDefault(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menable_sparse_data_optim)\n",
       "\u001B[1;32m   1357\u001B[0m predict_func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_predict_func()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e3d10686-b892-4abb-a373-bf46e7d14bca/lib/python3.9/site-packages/xgboost/spark/core.py:1229\u001B[0m, in \u001B[0;36m_SparkXGBModel._get_feature_col\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1222\u001B[0m     \u001B[38;5;66;03m# 1. The model was trained by features_cols, but the dataset doesn't contain\u001B[39;00m\n",
       "\u001B[1;32m   1223\u001B[0m     \u001B[38;5;66;03m#       all the columns specified by features_cols, so we need to check if\u001B[39;00m\n",
       "\u001B[1;32m   1224\u001B[0m     \u001B[38;5;66;03m#       the dataframe has the featuresCol\u001B[39;00m\n",
       "\u001B[1;32m   1225\u001B[0m     \u001B[38;5;66;03m# 2. The model was trained by featuresCol, and the predicted dataset must contain\u001B[39;00m\n",
       "\u001B[1;32m   1226\u001B[0m     \u001B[38;5;66;03m#       featuresCol column.\u001B[39;00m\n",
       "\u001B[1;32m   1227\u001B[0m     feature_col_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1228\u001B[0m     features_col\u001B[38;5;241m.\u001B[39mappend(\n",
       "\u001B[0;32m-> 1229\u001B[0m         \u001B[43m_validate_and_convert_feature_col_as_array_col\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1230\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrDefault\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeaturesCol\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1231\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1232\u001B[0m     )\n",
       "\u001B[1;32m   1233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m features_col, feature_col_names\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e3d10686-b892-4abb-a373-bf46e7d14bca/lib/python3.9/site-packages/xgboost/spark/core.py:522\u001B[0m, in \u001B[0;36m_validate_and_convert_feature_col_as_array_col\u001B[0;34m(dataset, features_col_name)\u001B[0m\n",
       "\u001B[1;32m    516\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_and_convert_feature_col_as_array_col\u001B[39m(\n",
       "\u001B[1;32m    517\u001B[0m     dataset: DataFrame, features_col_name: \u001B[38;5;28mstr\u001B[39m\n",
       "\u001B[1;32m    518\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Column:\n",
       "\u001B[1;32m    519\u001B[0m     \u001B[38;5;124;03m\"\"\"It handles\u001B[39;00m\n",
       "\u001B[1;32m    520\u001B[0m \u001B[38;5;124;03m    1. Convert vector type to array type\u001B[39;00m\n",
       "\u001B[1;32m    521\u001B[0m \u001B[38;5;124;03m    2. Cast to Array(Float32)\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 522\u001B[0m     features_col_datatype \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeatures_col_name\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mdataType\n",
       "\u001B[1;32m    523\u001B[0m     features_col \u001B[38;5;241m=\u001B[39m col(features_col_name)\n",
       "\u001B[1;32m    524\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(features_col_datatype, ArrayType):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:836\u001B[0m, in \u001B[0;36mStructType.__getitem__\u001B[0;34m(self, key)\u001B[0m\n",
       "\u001B[1;32m    834\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m field\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m key:\n",
       "\u001B[1;32m    835\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m field\n",
       "\u001B[0;32m--> 836\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo StructField named \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(key))\n",
       "\u001B[1;32m    837\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mint\u001B[39m):\n",
       "\u001B[1;32m    838\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'No StructField named features'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-1882788124453729>:6\u001B[0m\n\u001B[1;32m      4\u001B[0m xgb_regressor \u001B[38;5;241m=\u001B[39m SparkXGBRegressor(features_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, label_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m xgb_model \u001B[38;5;241m=\u001B[39m xgb_regressor\u001B[38;5;241m.\u001B[39mfit(train_data_yt)\n\u001B[0;32m----> 6\u001B[0m xgb_predictions \u001B[38;5;241m=\u001B[39m xgb_model\u001B[38;5;241m.\u001B[39mtransform(yt_cleaned_test)\n\u001B[1;32m      8\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m RegressionEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m xgb_rmse \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(xgb_predictions)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e3d10686-b892-4abb-a373-bf46e7d14bca/lib/python3.9/site-packages/xgboost/spark/core.py:1354\u001B[0m, in \u001B[0;36m_SparkXGBModel._transform\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m   1349\u001B[0m     base_margin_col \u001B[38;5;241m=\u001B[39m col(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetOrDefault(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_margin_col))\u001B[38;5;241m.\u001B[39malias(\n\u001B[1;32m   1350\u001B[0m         alias\u001B[38;5;241m.\u001B[39mmargin\n\u001B[1;32m   1351\u001B[0m     )\n\u001B[1;32m   1352\u001B[0m has_base_margin \u001B[38;5;241m=\u001B[39m base_margin_col \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1354\u001B[0m features_col, feature_col_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_feature_col\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1355\u001B[0m enable_sparse_data_optim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetOrDefault(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menable_sparse_data_optim)\n\u001B[1;32m   1357\u001B[0m predict_func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_predict_func()\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e3d10686-b892-4abb-a373-bf46e7d14bca/lib/python3.9/site-packages/xgboost/spark/core.py:1229\u001B[0m, in \u001B[0;36m_SparkXGBModel._get_feature_col\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1222\u001B[0m     \u001B[38;5;66;03m# 1. The model was trained by features_cols, but the dataset doesn't contain\u001B[39;00m\n\u001B[1;32m   1223\u001B[0m     \u001B[38;5;66;03m#       all the columns specified by features_cols, so we need to check if\u001B[39;00m\n\u001B[1;32m   1224\u001B[0m     \u001B[38;5;66;03m#       the dataframe has the featuresCol\u001B[39;00m\n\u001B[1;32m   1225\u001B[0m     \u001B[38;5;66;03m# 2. The model was trained by featuresCol, and the predicted dataset must contain\u001B[39;00m\n\u001B[1;32m   1226\u001B[0m     \u001B[38;5;66;03m#       featuresCol column.\u001B[39;00m\n\u001B[1;32m   1227\u001B[0m     feature_col_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1228\u001B[0m     features_col\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m-> 1229\u001B[0m         \u001B[43m_validate_and_convert_feature_col_as_array_col\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1230\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrDefault\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeaturesCol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1231\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1232\u001B[0m     )\n\u001B[1;32m   1233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m features_col, feature_col_names\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e3d10686-b892-4abb-a373-bf46e7d14bca/lib/python3.9/site-packages/xgboost/spark/core.py:522\u001B[0m, in \u001B[0;36m_validate_and_convert_feature_col_as_array_col\u001B[0;34m(dataset, features_col_name)\u001B[0m\n\u001B[1;32m    516\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_and_convert_feature_col_as_array_col\u001B[39m(\n\u001B[1;32m    517\u001B[0m     dataset: DataFrame, features_col_name: \u001B[38;5;28mstr\u001B[39m\n\u001B[1;32m    518\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Column:\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;124;03m\"\"\"It handles\u001B[39;00m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;124;03m    1. Convert vector type to array type\u001B[39;00m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;124;03m    2. Cast to Array(Float32)\"\"\"\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m     features_col_datatype \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeatures_col_name\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mdataType\n\u001B[1;32m    523\u001B[0m     features_col \u001B[38;5;241m=\u001B[39m col(features_col_name)\n\u001B[1;32m    524\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(features_col_datatype, ArrayType):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:836\u001B[0m, in \u001B[0;36mStructType.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    834\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m field\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m key:\n\u001B[1;32m    835\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m field\n\u001B[0;32m--> 836\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo StructField named \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(key))\n\u001B[1;32m    837\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m    838\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\n\u001B[0;31mKeyError\u001B[0m: 'No StructField named features'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'No StructField named features'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "xgb_regressor = SparkXGBRegressor(features_col=\"features\", label_col='label')\n",
    "xgb_model = xgb_regressor.fit(train_data_yt)\n",
    "xgb_predictions = xgb_model.transform(train_data_yt)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "xgb_rmse = evaluator.evaluate(xgb_predictions)\n",
    "print(f\"XGBoost RMSE: {xgb_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f40b98-f735-470c-83e6-27eb143131f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 349.23459891933686\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "xgb_predictions = xgb_model.transform(train_data_yt)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "xgb_rmse = evaluator.evaluate(xgb_predictions)\n",
    "print(f\"XGBoost RMSE: {xgb_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee17a96-47fe-409b-9699-94bd853ea8ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate set RMSE: 359.77156179780553\n"
     ]
    }
   ],
   "source": [
    "val_predictions_yt = xgb_model.transform(val_data_yt)\n",
    "\n",
    "val_rmse = evaluator.evaluate(val_predictions_yt)\n",
    "print(f\"Validate set RMSE: {val_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b299dddb-76aa-4aa6-b4f6-bd5a1de96471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 520.5412869879067\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(yt_cleaned_test)\n",
    "yt_cleaned_test = pipeline_model.transform(yt_cleaned_test)\n",
    "\n",
    "test_predictions_yt = xgb_model.transform(yt_cleaned_test)\n",
    "\n",
    "test_rmse = evaluator.evaluate(test_predictions_yt)\n",
    "print(f\"Test set RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51dee689-bb55-4934-9521-bc6c759579dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DOLocationID: long (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- trip_distance_km: double (nullable = true)\n |-- trip_duration: double (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- hour: integer (nullable = true)\n |-- weekday: integer (nullable = true)\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "train_data_yt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7290659-1dc1-4d27-9402-8fdbe132c682",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 4 - Yellow taxi dataset (2021~2022.9) with GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768cb3e3-c369-45d1-a982-699425727d60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.io.IOException: Connection failed\n",
       "\tat com.databricks.rpc.Jetty9Client$$anon$1.handleError(Jetty9Client.scala:845)\n",
       "\tat com.databricks.rpc.Jetty9Client$$anon$1.onFailure(Jetty9Client.scala:772)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:197)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:189)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.notifyFailureComplete(HttpExchange.java:275)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.abort(HttpExchange.java:247)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpConversation.abort(HttpConversation.java:164)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpRequest.abort(HttpRequest.java:821)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.abort(HttpDestination.java:559)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.failed(HttpDestination.java:313)\n",
       "\tat com.databricks.rpc.AbstractConnectionPool$1.failed(AbstractConnectionPool.java:161)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n",
       "\tat com.databricks.rpc.Jetty9Client$DatabricksHttpDestinationOverHTTP$$anon$2.failed(Jetty9Client.scala:1691)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpClient$1$1.failed(HttpClient.java:660)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport.connectFailed(AbstractConnectorHttpClientTransport.java:138)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport$ClientSelectorManager.connectionFailed(AbstractConnectorHttpClientTransport.java:188)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$Connect.failed(ManagedSelector.java:966)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:369)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1700(ManagedSelector.java:65)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:676)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:535)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:173)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:125)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$3(InstrumentedQueuedThreadPool.scala:173)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:125)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:167)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
       "Caused by: java.net.ConnectException: Connection refused\n",
       "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
       "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:347)\n",
       "\t... 26 more"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.io.IOException: Connection failed\n\tat com.databricks.rpc.Jetty9Client$$anon$1.handleError(Jetty9Client.scala:845)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onFailure(Jetty9Client.scala:772)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:197)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:189)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.notifyFailureComplete(HttpExchange.java:275)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.abort(HttpExchange.java:247)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpConversation.abort(HttpConversation.java:164)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpRequest.abort(HttpRequest.java:821)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.abort(HttpDestination.java:559)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.failed(HttpDestination.java:313)\n\tat com.databricks.rpc.AbstractConnectionPool$1.failed(AbstractConnectionPool.java:161)\n\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n\tat com.databricks.rpc.Jetty9Client$DatabricksHttpDestinationOverHTTP$$anon$2.failed(Jetty9Client.scala:1691)\n\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpClient$1$1.failed(HttpClient.java:660)\n\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport.connectFailed(AbstractConnectorHttpClientTransport.java:138)\n\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport$ClientSelectorManager.connectionFailed(AbstractConnectorHttpClientTransport.java:188)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$Connect.failed(ManagedSelector.java:966)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:369)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1700(ManagedSelector.java:65)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:676)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:535)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:173)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:125)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$3(InstrumentedQueuedThreadPool.scala:173)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:125)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:167)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:347)\n\t... 26 more\n",
       "errorSummary": "Internal error. Attach your notebook to a different compute or restart the current compute.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol='label', maxIter=50)\n",
    "\n",
    "gbt_model = gbt.fit(train_data_yt)\n",
    "gbt_predictions = gbt_model.transform(train_data_yt)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "gbt_rmse = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"GBT Train RMSE: {gbt_rmse}\")\n",
    "\n",
    "val_predictions_yt = gbt_model.transform(val_data_yt)\n",
    "\n",
    "val_rmse = evaluator.evaluate(val_predictions_yt)\n",
    "print(f\"GBT Validate set RMSE: {val_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "565a9ee2-0a03-4172-9646-1fcce57afdcc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 5 - Yellow taxi dataset (2021~2022.9) with RamdomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75dd4218-3c2c-4e5a-8109-53fd96989726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf Train RMSE: 467.53106056694156\nrf Validate set RMSE: 470.41473282095524\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='label', numTrees=5)\n",
    "\n",
    "rf_model = rf.fit(train_data_yt)\n",
    "rf_predictions = rf_model.transform(train_data_yt)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "print(f\"rf Train RMSE: {rf_rmse}\")\n",
    "\n",
    "val_predictions_yt = rf_model.transform(val_data_yt)\n",
    "\n",
    "val_rmse = evaluator.evaluate(val_predictions_yt)\n",
    "print(f\"rf Validate set RMSE: {val_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c87c9e-e70a-485b-88e5-7e4508696f60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 623.1471469120612\n"
     ]
    }
   ],
   "source": [
    "test_predictions_yt = rf_model.transform(yt_cleaned_test)\n",
    "\n",
    "test_rmse = evaluator.evaluate(test_predictions_yt)\n",
    "print(f\"Test set RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4804b88b-7a93-4951-be65-d4a5233ab5bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 6 - NYC taxi dataset (2021~2022.9) with XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67634555-360e-46a4-9862-dfe51c36dec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nyc_cleaned = nyc_taxi\n",
    "\n",
    "drop_columns = [\"mta_tax\", \"tolls_amount\", \"store_and_fwd_flag\", \"improvement_surcharge\", \"RatecodeID\" ,\"congestion_surcharge\", \"airport_fee\", \"speed_kmh\", \"trip_type\", \"ehail_fee\", \"payment_type\", \"VendorID\", \"fare_amount\"]\n",
    "                \n",
    "drop_point_columns = ['pickup_borough', 'pickup_zone', 'pickup_service_zone', 'dropoff_borough', 'dropoff_zone', 'dropoff_service_zone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5a2676-ccaa-43d1-89dd-7c348c6fbd3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nyc_cleaned = nyc_cleaned.drop(*drop_columns)\n",
    "nyc_cleaned = nyc_cleaned.drop(*drop_point_columns)\n",
    "nyc_cleaned = nyc_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288c7ba2-3af3-422a-919c-e9c8c49c3548",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, hour, dayofweek, expr\n",
    "\n",
    "nyc_cleaned = nyc_cleaned.withColumn(\"year\", year(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"month\", month(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"hour\", hour(\"pickup_datetime\")) \\\n",
    "       .withColumn(\"weekday\", dayofweek(\"pickup_datetime\"))\n",
    "\n",
    "nyc_cleaned = nyc_cleaned.drop(\"pickup_datetime\", \"dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f752bd-a982-4754-b4b2-d5aec5fb01b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nyc_cleaned_2021 = nyc_cleaned.filter((col(\"year\") == 2021) | ((col(\"year\") == 2022) & (col(\"month\") < 10)))\n",
    "nyc_cleaned_2019 = nyc_cleaned.filter((col(\"year\") >= 2019) & ((col(\"year\") <= 2020)))\n",
    "nyc_cleaned_2016 = nyc_cleaned.filter((col(\"year\") >= 2016) & ((col(\"year\") <= 2018)))\n",
    "nyc_cleaned_2015 = nyc_cleaned.filter((col(\"year\") == 2015))\n",
    "\n",
    "# Test data (October, November, December 2022)\n",
    "nyc_cleaned_test = nyc_cleaned.filter((col(\"year\") == 2022) & (col(\"month\").between(10, 12)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ac7c72-4b11-4cc4-a1c0-c0fc02e71398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nyc_cleaned_2021 = nyc_cleaned_2021.drop(\"year\")\n",
    "nyc_cleaned_2015 = nyc_cleaned_2015.drop(\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b51ad15-7ee7-471b-a09b-ff4c5babee4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nyc_cleaned_test = nyc_cleaned_test.drop(\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2f279b-6534-4342-a4cf-63330faf7322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DOLocationID: long (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- trip_distance_km: double (nullable = true)\n |-- trip_duration: double (nullable = true)\n |-- taxi_type: string (nullable = true)\n |-- month: integer (nullable = true)\n |-- hour: integer (nullable = true)\n |-- weekday: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "nyc_cleaned_2021.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec694f7-2caf-45d6-9286-e0c6f532be3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = ['taxi_type']\n",
    "num_cols2 = [field.name for field in nyc_cleaned_2021.schema.fields if field.dataType.typeName() in [\"integer\", \"double\", \"long\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a1b6db-1955-447d-b733-422bd5c0566c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "stages = []\n",
    "\n",
    "for cat_col in cat_cols:\n",
    "    col_indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_ind\")\n",
    "    col_encoder = OneHotEncoder(inputCols=[f\"{cat_col}_ind\"], outputCols=[f\"{cat_col}_ohe\"])\n",
    "    stages += [col_indexer, col_encoder]\n",
    "    \n",
    "cat_cols_ohe = [f\"{cat_col}_ohe\" for cat_col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e279d398-8d0b-474d-b7d1-97daa1527f60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "target_indexer = StringIndexer(inputCol='total_amount', outputCol='label')\n",
    "\n",
    "assembler = VectorAssembler(inputCols= cat_cols_ohe+num_cols2, outputCol=\"features\")\n",
    "\n",
    "stages += [target_indexer, assembler]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7748cab9-4dc5-4fae-a7e2-9518ed4e2778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(nyc_cleaned_2021)\n",
    "nyc_cleaned_2021 = pipeline_model.transform(nyc_cleaned_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e56deed-5db2-4c1a-a1d0-0f3a0d3cf961",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nt_train, nt_val = nyc_cleaned_2021.randomSplit([0.8, 0.2], seed=8)\n",
    "\n",
    "train_data_nt = nt_train.sample(fraction=0.1)\n",
    "val_data_nt = nt_val.sample(fraction=0.1)\n",
    "\n",
    "train_data_nt = train_data_nt.repartition(10)\n",
    "val_data_nt = val_data_nt.repartition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c030f53a-d240-4e8c-a603-66b4375fa438",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 622.1433566463232\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Calculate the baseline average total_amount\n",
    "baseline_value = train_data_nt.select(avg(\"total_amount\")).first()[0]\n",
    "\n",
    "# Create a DataFrame with baseline predictions\n",
    "baseline_predictions = train_data_nt.withColumn(\"prediction\", lit(baseline_value))\n",
    "\n",
    "# Calculate RMSE for baseline model\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "baseline_rmse = evaluator.evaluate(baseline_predictions)\n",
    "\n",
    "# Print the baseline RMSE\n",
    "print(f\"Baseline RMSE: {baseline_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53717419-6a29-4d82-a53c-4a40f97da620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 23:23:16,256 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-01 23:40:45,101 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 355.40350778784403\n"
     ]
    }
   ],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "xgb_regressor = SparkXGBRegressor(features_col=\"features\", label_col='label')\n",
    "xgb_model = xgb_regressor.fit(train_data_nt)\n",
    "xgb_predictions = xgb_model.transform(train_data_nt)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "xgb_rmse = evaluator.evaluate(xgb_predictions)\n",
    "print(f\"XGBoost RMSE: {xgb_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40891d5-920a-4fc6-abba-1eb3a19826df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 547.1826172095356\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(nyc_cleaned_test)\n",
    "test_data_nt = pipeline_model.transform(nyc_cleaned_test)\n",
    "\n",
    "test_predictions_yt = xgb_model.transform(test_data_nt)\n",
    "\n",
    "test_rmse = evaluator.evaluate(test_predictions_yt)\n",
    "print(f\"Test set RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64cb9d81-8d0a-4989-8349-f4d5c48ff4aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 7 - NYC taxi dataset (2021~2022.9) with XGB (Add more train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe2966b-3005-4696-983a-d6b5647b3cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nt_train, nt_val = nyc_cleaned_2021.randomSplit([0.8, 0.2], seed=8)\n",
    "\n",
    "train_data_nt2 = nt_train.sample(fraction=0.5)\n",
    "val_data_nt2 = nt_val.sample(fraction=0.5)\n",
    "\n",
    "train_data_nt2 = train_data_nt2.repartition(10)\n",
    "val_data_nt2 = val_data_nt2.repartition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12df6fa9-4617-4205-87d2-43111a430e63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train_data_nt2)\n",
    "train_data_nt2 = pipeline_model.transform(train_data_nt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2aec54e-caf8-498f-8cfd-e910550c044a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 05:16:34,589 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-02 05:36:34,642 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 354.62711950168637\n"
     ]
    }
   ],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "xgb_regressor = SparkXGBRegressor(features_col=\"features\", label_col='label')\n",
    "xgb_model2 = xgb_regressor.fit(train_data_nt2)\n",
    "xgb_predictions2 = xgb_model2.transform(train_data_nt2)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "xgb_rmse2 = evaluator.evaluate(xgb_predictions2)\n",
    "print(f\"XGBoost RMSE: {xgb_rmse2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf89b06-4051-4571-a803-dcd25b8db438",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 520.1583918296437\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(nyc_cleaned_test)\n",
    "test_data_nt = pipeline_model.transform(nyc_cleaned_test)\n",
    "\n",
    "test_predictions_nt = xgb_model2.transform(test_data_nt)\n",
    "\n",
    "test_rmse = evaluator.evaluate(test_predictions_nt)\n",
    "print(f\"Test set RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e13918b-af7e-4642-9735-e1f388a1be7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 8 - NYC taxi dataset (2015~2016) with XGB model above to validate model's performance \n",
    "for the past data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a7c82e2-db0e-40cc-89b0-87e1b5e8dac6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nyc_cleaned_2015 = nyc_cleaned_2015.drop(\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8677aed6-dae9-47bc-9eef-3ff3ff527332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nt_train3, nt_val3 = nyc_cleaned_2015.randomSplit([0.8, 0.2], seed=8)\n",
    "\n",
    "train_data_nt3 = nt_train3.sample(fraction=0.2)\n",
    "val_data_nt3 = nt_val3.sample(fraction=0.2)\n",
    "\n",
    "train_data_nt3 = train_data_nt3.repartition(10)\n",
    "val_data_nt3 = val_data_nt3.repartition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fec2787-b1f8-4e33-b358-9267d852d018",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2015 dataset RMSE: 338.9996428306595\n"
     ]
    }
   ],
   "source": [
    "# Validate 2015 NYC Taxi data with Trained model from 2021~2022 \n",
    "\n",
    "pipeline_model = pipeline.fit(train_data_nt3)\n",
    "train_data_nt_2015 = pipeline_model.transform(train_data_nt3)\n",
    "\n",
    "train_predictions_nt_2015 = xgb_model2.transform(train_data_nt_2015)\n",
    "\n",
    "train_rmse_2015 = evaluator.evaluate(train_predictions_nt_2015)\n",
    "print(f\"Train 2015 dataset RMSE: {train_rmse_2015}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f6dd61-59c5-46cc-b02e-bad1025419d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate 2015 dataset RMSE: 331.43194364837166\n"
     ]
    }
   ],
   "source": [
    "# Validate 2015 NYC Taxi data with Trained XGB model from 2021~2022 (10% sample data)\n",
    "\n",
    "pipeline_model = pipeline.fit(val_data_nt3)\n",
    "val_data_nt_2015 = pipeline_model.transform(val_data_nt3)\n",
    "\n",
    "val_predictions_nt_2015 = xgb_model2.transform(val_data_nt_2015)\n",
    "\n",
    "val_rmse_2015 = evaluator.evaluate(val_predictions_nt_2015)\n",
    "print(f\"validate 2015 dataset RMSE: {val_rmse_2015}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dec7836-b794-45ce-a1ef-01e5d790f710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Experiment 9 - XGB Hyperparameter tuning (max_depth, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81ba00b-73a3-407c-a340-b776a457ba2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 13:13:37,320 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 13:32:35,662 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 13:53:16,628 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 13:57:00,969 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 13:57:58,352 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 14:01:01,645 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 14:01:57,940 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 14:05:45,895 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 14:06:45,934 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 14:10:24,323 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 14:11:25,314 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 14:14:51,727 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 14:15:55,357 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 14:19:50,220 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 14:21:06,208 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 14:25:13,306 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 14:26:21,784 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 14:30:22,698 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 14:31:55,416 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 14:53:06,837 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:13:22,452 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 15:16:36,868 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:17:32,593 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 15:20:36,764 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:21:32,297 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 15:25:20,572 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:26:21,033 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 15:29:52,045 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:30:55,896 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 15:34:30,582 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:35:33,529 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 15:39:47,423 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:40:55,766 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 15:45:00,419 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:46:11,963 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 15:50:15,436 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 15:51:26,211 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 16:10:09,157 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 16:32:29,700 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 16:35:56,649 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 16:36:53,955 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 16:39:56,406 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 16:40:51,127 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 16:44:33,652 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 16:45:36,300 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 16:49:11,225 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 16:50:12,061 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 16:53:52,687 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 16:54:56,101 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 16:59:09,693 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 17:00:25,768 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 17:04:35,347 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 17:05:47,341 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 17:09:59,453 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-10-03 17:25:33,824 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-10-03 17:45:20,868 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-457186220349137>:30\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Print the best parameters\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest Parameters:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMax Depth: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_xgb_model\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetMaxDepth()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLearning Rate (eta): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_xgb_model\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetEta()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSubsample: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_xgb_model\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetSubsample()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'SparkXGBRegressorModel' object has no attribute '_java_obj'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-457186220349137>:30\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Print the best parameters\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest Parameters:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMax Depth: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_xgb_model\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetMaxDepth()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLearning Rate (eta): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_xgb_model\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetEta()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSubsample: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_xgb_model\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetSubsample()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\n\u001B[0;31mAttributeError\u001B[0m: 'SparkXGBRegressorModel' object has no attribute '_java_obj'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'SparkXGBRegressorModel' object has no attribute '_java_obj'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "xgb_regressor = SparkXGBRegressor(features_col=\"features\", label_col='label')\n",
    "\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(xgb_regressor.max_depth, [3, 5, 7])  # Tree depth\n",
    "              .addGrid(xgb_regressor.learning_rate, [0.1, 0.2, 0.3])  # Learning rate\n",
    "              .build())\n",
    "\n",
    "# Define an evaluator\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Set up cross-validation with 3 folds\n",
    "crossval = CrossValidator(estimator=xgb_regressor,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Fit the model with cross-validation\n",
    "cv_model = crossval.fit(train_data_nt2)\n",
    "\n",
    "# Get the best model from cross-validation\n",
    "best_xgb_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b88ad89-7caa-4fbc-a150-1e97d2c7081c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned XGBoost RMSE: 342.6183663171135\n"
     ]
    }
   ],
   "source": [
    "xgb_predictions = best_xgb_model.transform(train_data_nt2)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "xgb_rmse = evaluator.evaluate(xgb_predictions)\n",
    "print(f\"Tuned XGBoost RMSE: {xgb_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "896d23a2-32ea-4ef8-82ad-c18e3924f140",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\nMax Depth: 7\nLearning Rate (eta): 0.3\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters using the bestModel's params\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"Max Depth: {cv_model.bestModel.getOrDefault('max_depth')}\")\n",
    "print(f\"Learning Rate (eta): {cv_model.bestModel.getOrDefault('learning_rate')}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Part 3_feature_selection",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
